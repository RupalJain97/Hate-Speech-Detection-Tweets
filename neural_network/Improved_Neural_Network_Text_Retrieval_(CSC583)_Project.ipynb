{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ml-XsDlX50c8"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3NnpugV-uMb"
      },
      "outputs": [],
      "source": [
        "%cd \"/content/gdrive/MyDrive/CSC 583 Text Retrieval\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDeT80lS6TZl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ykki4j8J_9qV"
      },
      "source": [
        "## Prepare Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66-i45n7AHtU"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "\n",
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.version.cuda)\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nN-M9GH_9FM"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZ6bFTPTwi0h"
      },
      "source": [
        "### Loading dataset to fine tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWpxyO8tLeNu"
      },
      "outputs": [],
      "source": [
        "DATA2  = \"./Dataset/old_data_kaggle.csv\"\n",
        "#DATA2  = \"./Dataset/NN_improve_tweets.csv\"\n",
        "df2 = pd.read_csv(DATA2)\n",
        "df2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "959fAHD-znKY"
      },
      "outputs": [],
      "source": [
        "df2 = df2[['tweet','label']]\n",
        "#df2 = df2[['text','hate']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6o3wKPvLnHW"
      },
      "outputs": [],
      "source": [
        "# Define the label mapping\n",
        "label_map = {\n",
        "    'normal': 0,\n",
        "    'offensive': 1,\n",
        "    'hateful': 2\n",
        "}\n",
        "\n",
        "output_list = []\n",
        "for i in df2.index:\n",
        "  if df2.loc[i, 'hate'] == 1 or  df2.loc[i, 'hate'] == 2:\n",
        "    output_list.append([df2.loc[i, 'text'], 1])\n",
        "  else:\n",
        "    output_list.append([df2.loc[i, 'text'], 0])\n",
        "\n",
        "output_df = pd.DataFrame(output_list, columns=['text', 'hate'])\n",
        "print(output_df)\n",
        "df2 = output_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOjoSXyoBgfi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "train_df, dev_df, test_df =  np.split(df2.sample(frac=1, random_state=42),[int(.6*len(df2)), int(.8*len(df2))])\n",
        "print(train_df.shape, dev_df.shape, test_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFR5azqRN4tM"
      },
      "outputs": [],
      "source": [
        "#create custom dataset \n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "class TweetDataset(Dataset):\n",
        "\n",
        "    def __init__(self, encodings, labels):\n",
        "      self.encodings = encodings\n",
        "      self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['label'] = self.labels[idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3znMZRUiN7On"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 128\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained('vinai/bertweet-base', use_fast=True)\n",
        "\n",
        "TEXT_COL = \"tweet\"\n",
        "LABEL_COL = \"label\"\n",
        "\n",
        "# TEXT_COL = \"text\"\n",
        "# LABEL_COL = \"hate\"\n",
        "\n",
        "def create_dataset(dataframe, TEXT_COL, LABEL_COL):\n",
        "  inputs = {\n",
        "          \"input_ids\":[],\n",
        "          \"attention_mask\":[]\n",
        "        }\n",
        "\n",
        "  sents = dataframe[TEXT_COL].values.tolist()\n",
        "  for sent in sents:\n",
        "    tokenized_input = tokenizer(sent,max_length=MAX_LENGTH, padding='max_length', truncation = True)\n",
        "    inputs[\"input_ids\"].append(torch.tensor(tokenized_input[\"input_ids\"]))\n",
        "    inputs[\"attention_mask\"].append(torch.tensor(tokenized_input[\"attention_mask\"]))\n",
        "  # Create a TensorDataset from the input data\n",
        "  labels = torch.tensor(dataframe[LABEL_COL].values.tolist())\n",
        "  return TweetDataset(inputs, labels)\n",
        "\n",
        "train_dataset = create_dataset(train_df, TEXT_COL, LABEL_COL)\n",
        "dev_dataset = create_dataset(dev_df, TEXT_COL, LABEL_COL)\n",
        "test_dataset = create_dataset(test_df, TEXT_COL, LABEL_COL)\n",
        "print(test_dataset)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_i1AjDYYG5yS"
      },
      "outputs": [],
      "source": [
        "from transformers import BertConfig, BertTokenizer, BertForSequenceClassification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWlpPttiBp9x"
      },
      "source": [
        "### Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ff2NTeSEh7nB"
      },
      "outputs": [],
      "source": [
        "!pip install optuna\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dG0QNOSsk3OO"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import compute_class_weight\n",
        "import torch.nn as nn\n",
        "from transformers import Trainer, TrainingArguments\n",
        "if torch.cuda.is_available():    \n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")\n",
        "    \n",
        "def get_class_weights(dataframe,LABEL_COLUMN):\n",
        "  \"\"\"computes the class weight and returns a list to account for class imbalance \"\"\"\n",
        "  labels = torch.tensor(dataframe[LABEL_COLUMN].values.tolist())\n",
        "  class_weights=compute_class_weight( class_weight ='balanced',classes = np.unique(labels),y = labels.numpy())\n",
        "  class_weight_dict = dict(zip(np.unique(labels), class_weights))\n",
        "  total_class_weights =[]\n",
        "  for i in range(2):\n",
        "    if i not in class_weight_dict:\n",
        "      total_class_weights.append(1) #class_weight 1 for unseen labels\n",
        "    else:\n",
        "      total_class_weights.append(class_weight_dict[i])\n",
        "  total_class_weights =torch.tensor(total_class_weights,dtype=torch.float).to(device)\n",
        "  return total_class_weights\n",
        "\n",
        "def create_custom_trainer(class_weights):\n",
        "  \"\"\"creates custom trainer that accounts for class imbalance\"\"\"\n",
        "  class CustomTrainer(Trainer):\n",
        "      def compute_loss(self, model, inputs, return_outputs=False):\n",
        "          labels = inputs.get(\"labels\")\n",
        "          # forward pass\n",
        "          outputs = model(**inputs)\n",
        "          logits = outputs.get(\"logits\")\n",
        "          # compute custom loss \n",
        "          loss_fct = nn.CrossEntropyLoss(weight=class_weights)\n",
        "          loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "          return (loss, outputs) if return_outputs else loss\n",
        "  return CustomTrainer\n",
        "class_weights = get_class_weights(train_df,'label')\n",
        "CustomTrainer = create_custom_trainer(class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wfmFGfTnO-D"
      },
      "outputs": [],
      "source": [
        "from datasets import load_metric\n",
        "f1_metric =load_metric(\"f1\")\n",
        "def compute_metrics(eval_pred):\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=1)\n",
        "  return  f1_metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIIN7I47hvoq"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score   \n",
        "from transformers import AdamW, get_linear_schedule_with_warmup\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "import datetime\n",
        "from datetime import datetime\n",
        "import optuna\n",
        "\n",
        "num_labels = 2 # binary classification\n",
        "model = BertForSequenceClassification.from_pretrained('vinai/bertweet-base',num_labels = num_labels)\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # Define hyperparameters to tune\n",
        "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5,1e-3, log=True)\n",
        "    num_train_epochs = trial.suggest_int(\"num_train_epochs\", 3,3)\n",
        "    per_device_train_batch_size = trial.suggest_categorical(\"per_device_train_batch_size\", [8, 16, 32, 64])\n",
        "    per_device_eval_batch_size = per_device_train_batch_size \n",
        "\n",
        "    output_dir = \"./results_old/\"+str(datetime.now())\n",
        "    # Define training arguments\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=learning_rate,\n",
        "        per_device_train_batch_size=per_device_train_batch_size,\n",
        "        per_device_eval_batch_size=per_device_eval_batch_size,\n",
        "        num_train_epochs=num_train_epochs,\n",
        "        weight_decay=0.01,\n",
        "        push_to_hub=False,\n",
        "        logging_dir=\"./logs\",\n",
        "    )\n",
        "\n",
        "    # Train the model\n",
        "    trainer = CustomTrainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=dev_dataset,\n",
        "        compute_metrics=compute_metrics\n",
        "    )\n",
        "\n",
        "\n",
        "    trainer.train()\n",
        "    output = trainer.predict(dev_dataset)\n",
        "    predictions = np.argmax(output.predictions, axis=1)\n",
        "    f1 =  f1_metric.compute(predictions=predictions, references=output.label_ids)['f1']\n",
        "    print(f1)\n",
        "    return f1\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print(\"Number of finished trials: \", len(study.trials))\n",
        "print(\"Best trial:\")\n",
        "trial = study.best_trial\n",
        "print(\"  Value: \", trial.value)\n",
        "print(\"  Params: \")\n",
        "for key, value in trial.params.items():\n",
        "    print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train model with best performing hyperparameters"
      ],
      "metadata": {
        "id": "5DDBylocZndi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HejbwlbSQBIs"
      },
      "outputs": [],
      "source": [
        "import datetime\n",
        "from datetime import datetime\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from datasets import load_metric\n",
        "f1_metric =load_metric(\"f1\")\n",
        "def compute_metrics(eval_pred):\n",
        "  logits, labels = eval_pred\n",
        "  predictions = np.argmax(logits, axis=1)\n",
        "  return  f1_metric.compute(predictions=predictions, references=labels)\n",
        "# Define the training arguments\n",
        "\n",
        "learning_rate = 2e-5\n",
        "epochs = 3\n",
        "batch_size = 16\n",
        "#\n",
        "\n",
        "output_dir = \"./results_old/\"+str(datetime.now())\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    num_train_epochs=epochs,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    learning_rate = learning_rate,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy='steps',\n",
        "    load_best_model_at_end=True,\n",
        "     metric_for_best_model = 'f1',\n",
        "     eval_steps = 500,\n",
        "    do_train = True,\n",
        "  do_eval = True\n",
        ")\n",
        "\n",
        "num_labels = 2 # binary classification\n",
        "model = BertForSequenceClassification.from_pretrained('vinai/bertweet-base',num_labels = num_labels)\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "# Define the trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the finetuned model"
      ],
      "metadata": {
        "id": "Tg5OhWpmZuzM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hVqop88jWwLn"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "\n",
        "# Load the model and tokenizer\n",
        "num_labels = 2 # binary classification\n",
        "path = \"./results_old/2023-05-02 14:23:11.142440/checkpoint-3500\"\n",
        "model = BertForSequenceClassification.from_pretrained(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRmJGxxJj57i"
      },
      "outputs": [],
      "source": [
        "test_args = TrainingArguments(\n",
        "    output_dir = \"./prediction-results\",\n",
        "    do_train = False,\n",
        "    do_predict = True,\n",
        "    per_device_eval_batch_size = 16,   \n",
        ")\n",
        "test_trainer = Trainer( \n",
        "    model=model,\n",
        "    args=test_args,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PqhwfIK7kBfz"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate(test_dataset)\n",
        "results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get precision and recall scores\n",
        "import torch.nn.functional as F\n",
        "output = trainer.predict(test_dataset)\n",
        "probabilities = F.softmax(torch.from_numpy(output.predictions), dim=-1)\n",
        "pred_labels = np.argmax(output.predictions, axis=1)\n",
        "\n",
        "# get the gold labels of the test dataset\n",
        "gold_labels = []\n",
        "for x in test_df['hate'].values.tolist():\n",
        "  if x == 0 or x == 1:\n",
        "    gold_labels.append(x)\n",
        "  else:\n",
        "    gold_labels.append(1)"
      ],
      "metadata": {
        "id": "t2qLmlNSWRD-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(gold_labels, pred_labels))"
      ],
      "metadata": {
        "id": "FZF_iNExYalU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply the finetuned model to extract hate speech tweets and generate query"
      ],
      "metadata": {
        "id": "CkAsvh6fZ3jT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATA  = \"./Dataset/davidson_data.csv\"\n",
        "df = pd.read_csv(DATA)\n",
        "df"
      ],
      "metadata": {
        "id": "_gl4ln89aBmP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PpdFCp9h_h_U"
      },
      "outputs": [],
      "source": [
        "df = df[['tweet', 'hate_speech']]\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbpucoicXvw_"
      },
      "outputs": [],
      "source": [
        "from transformers import BertForSequenceClassification, BertTokenizer\n",
        "\n",
        "num_labels = 2 # binary classification\n",
        "# path = \"/content/gdrive/MyDrive/CSC 583 Text Retrieval/my_models/improved_bertweet\" # recent dataset finetuned \n",
        "path = \"/content/gdrive/MyDrive/CSC 583 Text Retrieval/combined_1_1_finetuned\"\n",
        "model = BertForSequenceClassification.from_pretrained(path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XFf-rU9_ufp"
      },
      "outputs": [],
      "source": [
        "# apply the model on the original dataset \n",
        "MAX_LENGTH = 128\n",
        "def create_dataset(dataframe):\n",
        "  inputs = {\n",
        "          \"input_ids\":[],\n",
        "          \"attention_mask\":[]\n",
        "        }\n",
        "\n",
        "  sents = dataframe['tweet'].values.tolist()\n",
        "  for sent in sents:\n",
        "    tokenized_input = tokenizer(sent,max_length=MAX_LENGTH, padding='max_length', truncation = True)\n",
        "    inputs[\"input_ids\"].append(torch.tensor(tokenized_input[\"input_ids\"]))\n",
        "    inputs[\"attention_mask\"].append(torch.tensor(tokenized_input[\"attention_mask\"]))\n",
        "\n",
        "  labels = torch.tensor([0]*dataframe.shape[0])\n",
        "\n",
        "  return TweetDataset(inputs, labels)\n",
        "\n",
        "test_dataset = create_dataset(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ysn4NZh-XjaV"
      },
      "outputs": [],
      "source": [
        "# Compute predictions using Trainer\n",
        "from transformers import Trainer, TrainingArguments\n",
        "output_dir=\"./prediction\"\n",
        "test_args = TrainingArguments(\n",
        "    output_dir = output_dir,\n",
        "    do_train = False,\n",
        "    do_predict = True,\n",
        "    per_device_eval_batch_size = 16,   \n",
        ")\n",
        "\n",
        "test_trainer = Trainer(model=model, args =test_args)\n",
        "output = test_trainer.predict(test_dataset)\n",
        "output\n",
        "# save prediction result \n",
        "import numpy as np\n",
        "#np.save('./combined_new_dataset_finetuned_davidson_prediction.npy', output.predictions) # save"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtYi12UgXlQB"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "probabilities = F.softmax(torch.from_numpy(output.predictions), dim=-1)\n",
        "pred_labels = np.argmax(output.predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fSFfFoVCXmv0"
      },
      "outputs": [],
      "source": [
        "# sort by high probability\n",
        "high_prob = torch.max(probabilities, dim = 1)\n",
        "print(high_prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpwPipE4XoLY"
      },
      "outputs": [],
      "source": [
        "sorted, index = high_prob.values.sort(descending=True)\n",
        "print(sorted, index) # we know the index of the tweets that have high prob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyuFb_pF5vpa"
      },
      "outputs": [],
      "source": [
        "# query generation\n",
        "def generate_hatespeech_query(index):\n",
        "  index_val = index.numpy().tolist()\n",
        "  sorted_val = sorted.numpy().tolist()\n",
        "  test_dataset_text = df['tweet'].values.tolist() # use davidson data to apply the improved model\n",
        "  print(index_val)\n",
        "  cnt = 0 \n",
        "\n",
        "  hate_speech_query = \"\"\n",
        "  for i in index_val:\n",
        "    if pred_labels[i] == 1:\n",
        "      \n",
        "      print(test_dataset_text[i], pred_labels[i], sorted_val[i])\n",
        "      processed_tweet = \" \".join(filter(lambda x:x[0]!='@', test_dataset_text[i].split()))\n",
        "      print(processed_tweet)\n",
        "      hate_speech_query += processed_tweet+ \" \"\n",
        "      cnt += 1\n",
        "    if cnt == 10:\n",
        "      break\n",
        "  \n",
        "  return hate_speech_query\n",
        "\n",
        "improved_query = generate_hatespeech_query(index)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SDJtY8l8QW3"
      },
      "outputs": [],
      "source": [
        "f = open(\"./combined_1_1_hatespeech_query\", \"w\")\n",
        "f.write(improved_query)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSKhz3jZSgxo"
      },
      "outputs": [],
      "source": [
        "# get the gold labels of the test dataset\n",
        "gold_labels = []\n",
        "for x in df['label'].values.tolist():\n",
        "  if x == 0 or x == 1:\n",
        "    gold_labels.append(x)\n",
        "  else:\n",
        "    gold_labels.append(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9XjQScmUZGH"
      },
      "outputs": [],
      "source": [
        "# get precision and recall score\n",
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(gold_labels, pred_labels))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}